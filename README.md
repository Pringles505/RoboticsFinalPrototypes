# AI-Vision Robotic Arm â€“ Color Picker Prototype

> **Prototipo de brazo robÃ³tico UR5e con visiÃ³n artificial y clasificaciÃ³n de objetos mediante GPT-4o Vision en simulaciÃ³n Webots.**

---

##  Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Authors](#authors)
- [Core Logic / Mechanics](#core-logic--mechanics)
- [System Architecture](#system-architecture)
  - [Directory Structure](#directory-structure)
  - [Component Configuration](#component-configuration)
- [Technical Specifications](#technical-specifications)
  - [Robot Specifications](#robot-specifications)
  - [Behavioral States](#behavioral-states)
  - [Motor Configuration](#motor-configuration)
- [Technical Implementation](#technical-implementation)
  - [Vision Pipeline](#vision-pipeline)
  - [Motion Control Algorithm](#motion-control-algorithm)
  - [Position Clamping Function](#position-clamping-function)
- [Installation & Setup](#installation--setup)
  - [Prerequisites](#prerequisites)
  - [Installation Steps](#installation-steps)
  - [Execution](#execution)
- [Configuration](#configuration)
  - [Adjustable Parameters](#adjustable-parameters)
  - [Tuning Guide](#tuning-guide)
- [Known Limitations](#known-limitations)
- [Contributing & Development Guidelines](#contributing--development-guidelines)

---

## Overview

Este proyecto implementa un **sistema robÃ³tico autÃ³nomo de pick-and-place** que integra visiÃ³n artificial con inteligencia artificial generativa para la clasificaciÃ³n de objetos por color. El sistema utiliza:

| TecnologÃ­a | PropÃ³sito |
|------------|-----------|
| **Webots R2025a** | SimulaciÃ³n fÃ­sica 3D del entorno robÃ³tico |
| **Universal Robots UR5e** | Brazo robÃ³tico industrial de 6 ejes |
| **Robotiq 3F Gripper** | Pinza de 3 dedos para manipulaciÃ³n de objetos |
| **OpenAI GPT-4o Vision** | Reconocimiento y clasificaciÃ³n de colores mediante IA |
| **OpenCV** | Captura y procesamiento de imÃ¡genes desde webcam |
| **Python 3.x** | Lenguaje de implementaciÃ³n del controlador |

El flujo operacional consiste en capturar una imagen de un objeto fÃ­sico mediante la webcam del usuario, enviarla a la API de OpenAI para identificaciÃ³n del color dominante, y ejecutar una secuencia de movimientos predefinidos para transportar el objeto virtual correspondiente a su contenedor designado.

---

## Features

###  Arquitectura
- **IntegraciÃ³n Webots-OpenAI**: Pipeline completo desde captura hasta ejecuciÃ³n motora
- **Auto-discovery de motores**: DetecciÃ³n automÃ¡tica de dispositivos del robot
- **Sistema de sensores de posiciÃ³n**: HabilitaciÃ³n dinÃ¡mica de encoders

###  ComunicaciÃ³n
- **API REST OpenAI**: EnvÃ­o de imÃ¡genes codificadas en Base64
- **Protocolo de Webots**: ComunicaciÃ³n sÃ­ncrona con timestep configurable
- **Interfaz HID**: Captura de eventos de teclado para trigger manual

###  MecÃ¡nicas
- **Gripper adaptativo**: Control seguro con lÃ­mites de posiciÃ³n
- **Keyframe animation**: Trayectorias intermedias para evitar colisiones
- **Multi-object support**: Mapeo de colores a objetos/destinos

---

## Authors

| Nombre | Rol |
|--------|-----|
| MascarÃ³ | Developer |
| Gonzalo | Developer |
| Marcos Cabrero | Developer |

---

## Core Logic / Mechanics

### Flujo de OperaciÃ³n Principal

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario       â”‚â”€â”€â”€â”€â–¶â”‚   Webcam         â”‚â”€â”€â”€â”€â–¶â”‚   OpenCV        â”‚
â”‚   (SPACEBAR)    â”‚     â”‚   Capture        â”‚     â”‚   Encode B64    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Robot Arm     â”‚â—€â”€â”€â”€â”€â”‚   Controller     â”‚â—€â”€â”€â”€â”€â”‚   OpenAI API    â”‚
â”‚   Pick & Place  â”‚     â”‚   Parse Color    â”‚     â”‚   GPT-4o Vision â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mapeo de Colores a Objetos

| Color Detectado | Objeto Virtual | PosiciÃ³n Destino |
|-----------------|----------------|------------------|
| `red` | `red_cube` | `red_cube_end` |
| `green` | `green_sphere` | `green_sphere_end` |
| `blue` | `blue_cylinder` | `blue_cylinder_end` |
| `none` | â€” | No action |

### Secuencia de Pick-and-Place

| Paso | AcciÃ³n | Wait Steps |
|------|--------|------------|
| 1 | Mover a posiciÃ³n `home` | 30 |
| 2 | Mover a posiciÃ³n `{object}_above` | 30 |
| 3 | Abrir gripper | 80 |
| 4 | Mover a posiciÃ³n `{object}` | 30 |
| 5 | Cerrar gripper | 80 |
| 6 | Mover a posiciÃ³n `{object}_end` | 150 |
| 7 | Abrir gripper | â€” |
| 8 | Retornar a `home` | 150 |

---

## System Architecture

### Directory Structure

```
RoboticsFinalPrototypes/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # DocumentaciÃ³n del proyecto
â”œâ”€â”€ ğŸ“„ .env.example                       # Plantilla de variables de entorno
â”œâ”€â”€ ğŸ“„ .gitignore                         # Archivos ignorados por Git
â”‚
â”œâ”€â”€ ğŸ“ controllers/
â”‚   â””â”€â”€ ğŸ“ auto_controller/
â”‚       â””â”€â”€ ğŸ auto_controller.py         # Controlador principal (215 LOC)
â”‚
â””â”€â”€ ğŸ“ worlds/
    â””â”€â”€ ğŸŒ color_picker_robot.wbt         # Mundo de simulaciÃ³n Webots
```

| Directorio | DescripciÃ³n |
|------------|-------------|
| `controllers/` | Contiene los controladores Python para Webots |
| `controllers/auto_controller/` | Controlador principal del sistema |
| `worlds/` | Archivos de mundo `.wbt` para la simulaciÃ³n |

### Component Configuration

#### Componentes de Hardware Simulado

| Componente | Modelo | EspecificaciÃ³n |
|------------|--------|----------------|
| **Brazo RobÃ³tico** | UR5e | 6 DOF, alcance 850mm |
| **Gripper** | Robotiq 3F | 3 dedos adaptativos |
| **CÃ¡mara** | Virtual Camera | 640Ã—480 px |
| **Arena** | RectangleArena | 3m Ã— 3m |

#### Objetos del Mundo

| Objeto | Color RGB | TamaÃ±o (m) | Masa (kg) |
|--------|-----------|------------|-----------|
| `red_cube` | `(1, 0, 0)` | 0.1 Ã— 0.1 Ã— 0.1 | 0.1 |
| `green_sphere` | `(0, 1, 0)` | 0.1 Ã— 0.1 Ã— 0.1 | 0.1 |
| `blue_cylinder` | `(0, 0, 1)` | 0.1 Ã— 0.1 Ã— 0.1 | 0.1 |
| `box_1`, `box_2`, `box_3` | `(0.8, 0.8, 0.8)` | 0.15 Ã— 0.15 Ã— 0.1 | â€” |

---

## Technical Specifications

### Robot Specifications

| ParÃ¡metro | Valor |
|-----------|-------|
| **Modelo** | Universal Robots UR5e |
| **Grados de Libertad** | 6 (+ gripper) |
| **Controlador** | `auto_controller.py` |
| **Timestep Base** | DinÃ¡mico (`robot.getBasicTimeStep()`) |
| **Velocidad Arm Motors** | 1.0 rad/s |
| **Velocidad Gripper Motors** | 0.5 rad/s |

### Behavioral States

| Estado | DescripciÃ³n | Trigger |
|--------|-------------|---------|
| **IDLE** | Robot en posiciÃ³n home, esperando input | Inicio de simulaciÃ³n |
| **CAPTURING** | Captura de imagen desde webcam | Tecla SPACEBAR |
| **ANALYZING** | Enviando imagen a OpenAI API | Post-captura |
| **PICKING** | Ejecutando secuencia de recogida | Color vÃ¡lido detectado |
| **PLACING** | Ejecutando secuencia de colocaciÃ³n | Objeto agarrado |
| **RETURNING** | Retornando a posiciÃ³n home | Post-colocaciÃ³n |

### Motor Configuration

#### Arm Motors (6 joints)

| Joint | PosiciÃ³n Home (rad) | DescripciÃ³n |
|-------|---------------------|-------------|
| Joint 0 | `0.00` | Base rotation |
| Joint 1 | `-1.57` | Shoulder |
| Joint 2 | `1.57` | Elbow |
| Joint 3 | `-1.57` | Wrist 1 |
| Joint 4 | `-1.57` | Wrist 2 |
| Joint 5 | `0.00` | Wrist 3 |

#### Gripper Configuration

| ParÃ¡metro | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| `GRIPPER_OPEN` | `-0.1` rad | PosiciÃ³n de apertura |
| `GRIPPER_CLOSE` | `0.6` rad | PosiciÃ³n de cierre |

---

## Technical Implementation

### Vision Pipeline

```python
# 1. Captura de frame desde webcam
ret, frame = self.webcam.read()

# 2. CodificaciÃ³n a Base64
_, buf = cv2.imencode(".jpg", frame)
image_b64 = base64.b64encode(buf).decode()

# 3. EnvÃ­o a OpenAI GPT-4o Vision
response = self.client.chat.completions.create(
    model="gpt-4o",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {
                "url": f"data:image/jpeg;base64,{image_b64}",
                "detail": "low"
            }}
        ]
    }],
    max_tokens=5,
    temperature=0
)
```

#### ParÃ¡metros de OpenAI API

| ParÃ¡metro | Valor | JustificaciÃ³n |
|-----------|-------|---------------|
| `model` | `gpt-4o` | Modelo con capacidades de visiÃ³n |
| `detail` | `low` | ReducciÃ³n de tokens/costo |
| `max_tokens` | `5` | Respuesta mÃ­nima (1 palabra) |
| `temperature` | `0` | Determinismo mÃ¡ximo |

### Motion Control Algorithm

La secuencia de movimiento utiliza **keyframe interpolation** para evitar colisiones:

```
Position Flow:
home â†’ {object}_above â†’ {object} â†’ {object}_end â†’ home
           â”‚               â”‚            â”‚
           â””â”€â”€ clearance â”€â”€â”´â”€â”€ pickup â”€â”€â”´â”€â”€ dropoff
```

#### Posiciones Predefinidas (radianes)

| PosiciÃ³n | J0 | J1 | J2 | J3 | J4 | J5 |
|----------|-------|-------|------|-------|-------|------|
| `home` | 0.00 | -1.57 | 1.57 | -1.57 | -1.57 | 0.00 |
| `red_cube` | 0.50 | -1.27 | 1.87 | -2.27 | -1.57 | 0.00 |
| `red_cube_above` | 0.50 | -1.27 | 1.57 | -2.07 | -1.57 | 0.00 |
| `red_cube_end` | -3.90 | -1.11 | 1.37 | -1.87 | -1.57 | 0.00 |
| `green_sphere` | -0.10 | -1.07 | 1.67 | -2.07 | -1.57 | 0.00 |
| `green_sphere_above` | -0.10 | -1.47 | 1.67 | -1.87 | -1.57 | 0.00 |
| `green_sphere_end` | -3.40 | -1.11 | 1.37 | -1.87 | -1.57 | 0.00 |
| `blue_cylinder` | -0.90 | -1.27 | 1.87 | -2.27 | -1.57 | 0.10 |
| `blue_cylinder_above` | -0.60 | -1.37 | 1.37 | -1.57 | -1.57 | 0.00 |
| `blue_cylinder_end` | -2.90 | -1.11 | 1.37 | -1.87 | -1.57 | 0.00 |

### Position Clamping Function

Para garantizar movimientos seguros dentro de los lÃ­mites fÃ­sicos del motor:

```python
def set_motor_position_safe(motor: Motor, target: float):
    """
    Aplica clamping a la posiciÃ³n objetivo dentro de los lÃ­mites del motor.
    
    Args:
        motor: Instancia del motor Webots
        target: PosiciÃ³n objetivo en radianes
    """
    mn = motor.getMinPosition()  # LÃ­mite inferior
    mx = motor.getMaxPosition()  # LÃ­mite superior

    if mn != float("-inf") and target < mn:
        target = mn
    if mx != float("inf") and target > mx:
        target = mx

    motor.setPosition(target)
```

---

## Installation & Setup

### Prerequisites

| Requisito | VersiÃ³n | Notas |
|-----------|---------|-------|
| **Webots** | R2025a | Simulador robÃ³tico |
| **Python** | â‰¥ 3.8 | Runtime del controlador |
| **OpenCV** | â‰¥ 4.5 | Procesamiento de imagen |
| **OpenAI SDK** | â‰¥ 1.0 | Cliente de API |
| **Webcam** | â€” | Hardware requerido |
| **API Key OpenAI** | â€” | Con acceso a GPT-4o |

### Installation Steps

```bash
# 1. Clonar el repositorio
git clone https://github.com/Pringles505/RoboticsFinalPrototypes.git
cd RoboticsFinalPrototypes

# 2. Instalar dependencias Python
pip install opencv-python numpy openai python-dotenv

# 3. Configurar API Key de OpenAI
cp .env.example .env
# Editar .env y aÃ±adir tu API Key:
# OPENAI_API_KEY=tu-api-key-aqui
```

> âš ï¸ **Importante**: El archivo `.env` estÃ¡ incluido en `.gitignore` para proteger tu API Key. Nunca subas credenciales al repositorio.

### Execution

```bash
# 1. Abrir Webots
webots worlds/color_picker_robot.wbt

# 2. Iniciar simulaciÃ³n (Play button o Ctrl+P)
# 3. La webcam se activarÃ¡ automÃ¡ticamente
# 4. Presionar SPACEBAR para capturar y procesar
```

---

## Configuration

### Adjustable Parameters

```python
# === OPENAI CONFIGURATION (.env file) ===
# OPENAI_API_KEY=tu-api-key     # API Key (REQUERIDO) - configurar en .env

# === WEBCAM CONFIGURATION ===
self.webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)   # Ancho de captura
self.webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Alto de captura

# === MOTOR VELOCITIES ===
for m in self.arm_motors:
    m.setVelocity(1.0)           # Velocidad del brazo (rad/s)
for g in self.gripper_motors:
    g.setVelocity(0.5)           # Velocidad del gripper (rad/s)

# === GRIPPER POSITIONS ===
self.GRIPPER_OPEN = -0.1         # PosiciÃ³n abierta
self.GRIPPER_CLOSE = 0.6         # PosiciÃ³n cerrada

# === TIMING ===
self.wait(steps=150)             # Steps de espera por defecto
self.wait(80)                    # Steps para operaciones de gripper
self.wait(30)                    # Steps para movimientos de brazo
```

### Tuning Guide

#### Ajuste de Velocidad

| Escenario | Arm Velocity | Gripper Velocity | Wait Steps |
|-----------|--------------|------------------|------------|
| **PrecisiÃ³n alta** | 0.5 | 0.3 | 200 |
| **Balanceado** | 1.0 | 0.5 | 150 |
| **Alta velocidad** | 2.0 | 1.0 | 80 |

#### CalibraciÃ³n de Posiciones

Para calibrar nuevas posiciones de objetos:

1. Ejecutar simulaciÃ³n en modo **pausa**
2. Usar el panel de **Position Sensors** de Webots
3. Mover manualmente el robot a la posiciÃ³n deseada
4. Leer valores de los sensores
5. Actualizar diccionario `self.positions`

---

## Known Limitations

| Comportamiento | Causa | MitigaciÃ³n |
|----------------|-------|------------|
| **Posiciones hardcodeadas** | No hay path planning dinÃ¡mico | Calibrar manualmente para nuevos objetos |
| **Sin detecciÃ³n de colisiones** | Trayectorias predefinidas | Usar posiciones `_above` intermedias |
| **Dependencia de iluminaciÃ³n** | GPT-4o sensible a condiciones de luz | Usar iluminaciÃ³n consistente |
| **Latencia de API** | Llamada HTTP a OpenAI | ~1-3s por detecciÃ³n |
| **Solo 3 colores** | Mapeo fijo en `color_to_object` | Extender diccionario para mÃ¡s colores |
| **Gripper genÃ©rico** | Valores de apertura/cierre fijos | Ajustar `GRIPPER_OPEN`/`GRIPPER_CLOSE` |
| **Webcam index 0** | Asume primera cÃ¡mara disponible | Cambiar Ã­ndice en `cv2.VideoCapture(n)` |
| **Sin recuperaciÃ³n de errores** | Fallos de API terminan ejecuciÃ³n | Implementar try/catch con reintentos |

---

## Contributing & Development Guidelines

### Git Flow

```bash
# 1. Fork del repositorio
# 2. Crear rama feature
git checkout -b feature/nueva-funcionalidad

# 3. Commits atÃ³micos
git commit -m "feat: descripciÃ³n concisa del cambio"

# 4. Push y Pull Request
git push origin feature/nueva-funcionalidad
```

### ConvenciÃ³n de Commits

| Prefijo | Uso |
|---------|-----|
| `feat:` | Nueva funcionalidad |
| `fix:` | CorrecciÃ³n de bug |
| `docs:` | Cambios en documentaciÃ³n |
| `refactor:` | RefactorizaciÃ³n de cÃ³digo |
| `test:` | AÃ±adir o modificar tests |

### Estilo de CÃ³digo

- **PEP 8** para Python
- **Type hints** recomendados
- **Docstrings** en formato Google
- **Nombres descriptivos** para variables y funciones

### Estructura de Nuevos Controladores

```python
"""
Docstring del mÃ³dulo.
"""
from controller import Robot, Motor

# Constants
CONSTANT_NAME = value

# Helper functions
def helper_function():
    """Docstring."""
    pass

# Main class
class ControllerName:
    """Docstring."""
    
    def __init__(self):
        pass
    
    def run(self):
        pass

if __name__ == "__main__":
    controller = ControllerName()
    controller.run()
```

---

## License

Este proyecto se proporciona como **prototipo experimental** con fines educativos bajo licencia **MIT**.

---

<p align="center">
  <b>IMMUNE Technology Institute â€“ Robotics Final Project</b><br>
  <i>AI-Powered Vision System for Robotic Manipulation</i>
</p>
